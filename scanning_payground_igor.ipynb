{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant packages\n",
    "import psycopg2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from astropy.table import Table\n",
    "from astropy.time import Time\n",
    "from astropy.io import ascii, fits\n",
    "from collections import OrderedDict\n",
    "\n",
    "from penquins import Kowalski\n",
    "from bson.json_util import loads, dumps\n",
    "import gzip\n",
    "import io\n",
    "from matplotlib.colors import LogNorm\n",
    "from astropy.stats import sigma_clipped_stats, mad_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Connect to the psql database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the secrets\n",
    "info = ascii.read('./db_access.csv', format='csv')\n",
    "info_db = info[info['db'] == 'db_kn_rt_user']\n",
    "db_kn = f\"host={info_db['host'][0]} dbname={info_db['dbname'][0]} \\\n",
    "port={info_db['port'][0]} user={info_db['user'][0]} \\\n",
    "password={info_db['password'][0]}\"\n",
    "\n",
    "# Connect\n",
    "con = psycopg2.connect(db_kn)\n",
    "cur = con.cursor()\n",
    "print(f\"Connected to the '{info_db['dbname'][0]}' database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tables\n",
    "There are multiple tables in the database. Here you can print the names of the tables and the names of the columns available in each table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Table names\n",
    "tables = pd.read_sql_query(\"SELECT table_name FROM information_schema.tables WHERE table_schema='public' \",con)\n",
    "\n",
    "for t in tables[\"table_name\"]:\n",
    "    print(f\"Table {t}:\")\n",
    "    q = pd.read_sql_query(f\"SELECT column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE TABLE_NAME = '{t}'\", con)\n",
    "    print(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting functions\n",
    "Compile the cells below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_triplet(alert, normalize: bool = False):\n",
    "    \"\"\"\n",
    "        Feed in alert packet\n",
    "    \"\"\"\n",
    "    cutout_dict = dict()\n",
    "\n",
    "    for cutout in ('science', 'template', 'difference'):\n",
    "        cutout_data = loads(dumps([alert[f'cutout{cutout.capitalize()}']['stampData']]))[0]\n",
    "\n",
    "        # unzip\n",
    "        with gzip.open(io.BytesIO(cutout_data), 'rb') as f:\n",
    "            with fits.open(io.BytesIO(f.read())) as hdu:\n",
    "                data = hdu[0].data\n",
    "                # replace nans with zeros\n",
    "                cutout_dict[cutout] = np.nan_to_num(data)\n",
    "                # normalize\n",
    "                if normalize:\n",
    "                    cutout_dict[cutout] /= np.linalg.norm(cutout_dict[cutout])\n",
    "\n",
    "        # pad to 63x63 if smaller\n",
    "        shape = cutout_dict[cutout].shape\n",
    "        if shape != (63, 63):\n",
    "            cutout_dict[cutout] = np.pad(cutout_dict[cutout], [(0, 63 - shape[0]), (0, 63 - shape[1])],\n",
    "                                         mode='constant', constant_values=1e-9)\n",
    "\n",
    "    triplet = np.zeros((63, 63, 3))\n",
    "    triplet[:, :, 0] = cutout_dict['science']\n",
    "    triplet[:, :, 1] = cutout_dict['template']\n",
    "    triplet[:, :, 2] = cutout_dict['difference']\n",
    "\n",
    "    return triplet\n",
    "\n",
    "def plot_triplet(tr):\n",
    "    fig = plt.figure(figsize=(8, 2), dpi=120)\n",
    "    ax1 = fig.add_subplot(131)\n",
    "    ax1.axis('off')\n",
    "    mean, median, std = sigma_clipped_stats(tr[:, :, 0])\n",
    "    ax1.imshow(tr[:, :, 0], vmin = median - 2*std, vmax = median + 3*std)\n",
    "    #ax1.imshow(tr[:, :, 0], origin='upper', cmap=plt.cm.bone, norm=LogNorm())\n",
    "    ax1.title.set_text('Science')\n",
    "    ax2 = fig.add_subplot(132)\n",
    "    ax2.axis('off')\n",
    "    mean, median, std = sigma_clipped_stats(tr[:, :, 1])\n",
    "    ax2.imshow(tr[:, :, 1], vmin = median - 2*std, vmax = median + 3*std)\n",
    "    #ax2.imshow(tr[:, :, 1], origin='upper', cmap=plt.cm.bone, norm=LogNorm())\n",
    "    ax2.title.set_text('Reference')\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    ax3.axis('off')\n",
    "    mean, median, std = sigma_clipped_stats(tr[:, :, 2])\n",
    "    ax3.imshow(tr[:, :, 2], vmin = median - 2*std, vmax = median + 3*std)\n",
    "    #ax3.imshow(tr[:, :, 2], origin='upper', cmap=plt.cm.bone)\n",
    "    ax3.title.set_text('Difference')\n",
    "    plt.show()\n",
    "\n",
    "def get_cutouts(name, username, password):\n",
    "    \"\"\"Query kowalski to get the candidate stamps\"\"\"\n",
    "    \n",
    "    k = Kowalski(username=username, password=password, verbose=False)\n",
    "\n",
    "    if type(name) == str:\n",
    "        list_names = [name]\n",
    "    elif type(name) == list:\n",
    "        list_names = name\n",
    "    else:\n",
    "        print(f\"{name} must be a list or a string\")\n",
    "        return None\n",
    "\n",
    "    q = {\"query_type\": \"find\",\n",
    "         \"query\": {\n",
    "                   \"catalog\": \"ZTF_alerts\",\n",
    "                   \"filter\": {\n",
    "                              'objectId': {'$in': list(list_names)}\n",
    "                              },\n",
    "                   \"projection\": {\n",
    "                                  \"objectId\": 1,\n",
    "                                  \"candidate.jd\": 1,\n",
    "                                  \"candidate.ra\": 1,\n",
    "                                  \"candidate.dec\": 1,\n",
    "                                  \"candidate.magpsf\": 1,\n",
    "                                  \"candidate.fid\": 1,\n",
    "                                  \"candidate.sigmapsf\": 1,\n",
    "                                  \"candidate.programid\": 1,\n",
    "                                  \"candidate.field\": 1,\n",
    "                                  \"candidate.rcid\": 1,\n",
    "                                  \"cutoutScience\": 1,\n",
    "                                  \"cutoutTemplate\": 1,\n",
    "                                  \"cutoutDifference\": 1,\n",
    "                                  }\n",
    "                       },\n",
    "         \"kwargs\": {\"hint\": \"objectId_1\"}\n",
    "         }\n",
    "\n",
    "    r = k.query(query=q)\n",
    "\n",
    "    if r['result_data']['query_result'] == []:\n",
    "        print(\"No candidates to be checked?\")\n",
    "        return None\n",
    "    else:\n",
    "        alerts = r['result_data']['query_result']\n",
    "\n",
    "    return alerts\n",
    "\n",
    "    \"\"\"\n",
    "        name = info[\"objectId\"]\n",
    "        cutouts[name] = {'science': r['result_data']['query_result'][0][\"cutoutScience\"],\n",
    "               'template': r['result_data']['query_result'][0][\"cutoutTemplate\"],\n",
    "               'difference': r['result_data']['query_result'][0][\"cutoutDifference\"]}\n",
    "    return cutouts\n",
    "    \"\"\"\n",
    "\n",
    "# Read the secrets\n",
    "secrets = ascii.read('secrets.csv', format='csv')\n",
    "username_kowalski = secrets['kowalski_user'][0]\n",
    "password_kowalski = secrets['kowalski_pwd'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dust_info(coords):\n",
    "    \"\"\"\n",
    "    Get coordinates as a SkyCoord object\n",
    "    and returns E(B-V) \n",
    "    \"\"\"\n",
    "    from dustmaps.planck import PlanckQuery\n",
    "    from dustmaps.sfd import SFDQuery\n",
    "    \n",
    "    planck = PlanckQuery()\n",
    "    ebv = planck(coords)\n",
    "    #print('E(B-V) = {:.3f} mag'.format(ebv))\n",
    "\n",
    "    return ebv\n",
    "\n",
    "\n",
    "def plot_lc(name, forced=True, stack=False, plot_alerts=True, save=False, reddening=False,\n",
    "            plot_cow=True, plot_gfo=True, plot_bulla=True, filtermatch = 'g',\n",
    "            plot_gw=False, inset=False, tr=None, writecsv=False):\n",
    "    '''Plot the light curve of a candidate'''\n",
    "\n",
    "    color_dict = {'g': 'green', 'r': 'red', 'i': 'y'}\n",
    "    forced_dict = {'1': 's', '0': 'o'}\n",
    "\n",
    "    if forced is False:\n",
    "        plot_alerts = True\n",
    "        lc = pd.DataFrame(columns=['jd', 'mag', 'mag_unc', 'filter', 'limmag', 'forced'])\n",
    "    else:\n",
    "        if stack is True:\n",
    "            table = 'lightcurve_stacked'        \n",
    "        elif forced is True:\n",
    "            table = 'lightcurve_forced'\n",
    "        lc = pd.read_sql_query(f\"SELECT jd, mag, mag_unc, filter, limmag FROM {table} \\\n",
    "WHERE name = '{name}'\", con)\n",
    "        lc[\"forced\"] = np.ones(len(lc))\n",
    "\n",
    "    if plot_alerts is True:\n",
    "        alerts = pd.read_sql_query(f\"SELECT jd, magpsf, sigmapsf, filter FROM lightcurve \\\n",
    "WHERE name = '{name}'\", con)\n",
    "        # Remove the alerts if they are way too many\n",
    "        if len(alerts) > 20:\n",
    "            print(\"TOO MANY ALERTS!!!! Not plotting them\")\n",
    "        for i, a in alerts.iterrows():\n",
    "            if len(alerts) > 20:\n",
    "                continue\n",
    "            # If the time difference between the alert and any forced phot is >5min, consider the alert\n",
    "            if lc.empty or np.min(np.abs(np.array(lc['jd']) - np.array(a['jd']))) > 5./60/60/24.:\n",
    "                #print(f\"Adding an alert for {name}\")\n",
    "                new_row = [a['jd'], a['magpsf'], a['sigmapsf'], a[\"filter\"], 99.0, 0]\n",
    "                new_row = pd.DataFrame([new_row], columns=['jd', 'mag', 'mag_unc', 'filter', 'limmag', 'forced'])\n",
    "                lc = lc.append([lc, new_row], ignore_index=True)\n",
    "\n",
    "    # Plot    \n",
    "    if plot_gw is True:\n",
    "        gw_info = {'ZTF19acbqtue': {'gw_name': 'S190930t',\n",
    "                                    'gw_time': Time('2019-09-30 14:34:07', format='iso')}                  \n",
    "                  }\n",
    "        if name in gw_info.keys():\n",
    "            t0 = gw_info[name]['gw_time'].jd\n",
    "            xlabel = gw_info[name]['gw_name']\n",
    "        else:\n",
    "            t0 = np.min(lc['jd'].values)\n",
    "            xlabel = Time(t0, format='jd').iso[0:1]\n",
    "    else:\n",
    "        if lc.empty:\n",
    "            print(f\"Empty light curve for {name} with forced={forced}, stack={stack}, plot_alerts={plot_alerts}\")\n",
    "            return\n",
    "        t0 = np.min(lc[lc['mag'] < 50]['jd'].values)\n",
    "        xlabel = Time(t0, format='jd').iso[0:19]\n",
    "\n",
    "    # Correct for Galactic extinction\n",
    "    if reddening is True:\n",
    "        # Get the coordinates\n",
    "        coords_tbl = pd.read_sql_query(f\"SELECT ra, dec FROM candidate \\\n",
    "                                WHERE name = '{name}'\", con)\n",
    "        coords = SkyCoord(ra=coords_tbl[\"ra\"][0].value*u.deg, dec=coords_tbl[\"dec\"][0].value*u.deg)\n",
    "        ebv = get_dust_info(coords)\n",
    "        \n",
    "        \"\"\"\n",
    "        # Dict with correction factor to go from ebv to A_lambda\n",
    "        corr_dict = [\"g\": 3.3, \"r\", \"i\"]\n",
    "        \"\"\"\n",
    "    \n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(9,6))\n",
    "    \n",
    "    for f in set(lc['filter']):\n",
    "        tf = lc[lc['filter'] == f]\n",
    "\n",
    "        \"\"\"\n",
    "        # Correct for the extinction\n",
    "        if reddening is True:\n",
    "            corr = ebv * corr_dict[f]\n",
    "            tf[\"mag\"] = tf[\"mag\"] - \n",
    "            tf[\"limmag\"] = tf[\"limmag\"] - \n",
    "        \"\"\"\n",
    "        tf_det = tf[tf['mag'] < 50.]\n",
    "        tf_ul = tf[tf['mag'] > 50.]\n",
    "        for isforced in [0,1]:\n",
    "            if isforced == 0:\n",
    "                label = f\"{f} alerts\"\n",
    "            else:\n",
    "                if stack is True:\n",
    "                    label = f\"{f} forced phot stacked\"\n",
    "                else:\n",
    "                    label = f\"{f} forced phot\"\n",
    "            tf_det2 = tf_det[tf_det['forced'] == isforced]\n",
    "            if len(tf_det2) == 0:\n",
    "                continue\n",
    "            ax1.errorbar(tf_det2['jd'].values - t0, tf_det2['mag'], yerr=tf_det2['mag_unc'],\n",
    "                         color=color_dict[f], markeredgecolor='k',\n",
    "                         fmt=forced_dict[str(int(isforced))], label=label)\n",
    "        if len(tf_ul) != 0:\n",
    "            ax1.errorbar(tf_ul['jd'].values - t0, tf_ul['limmag'], markeredgecolor=color_dict[f],\n",
    "                         markerfacecolor='w', fmt='v')\n",
    "            plt.plot([],[], 'kv', markeredgecolor='k', markerfacecolor='w', label='upper limits')\n",
    "  \n",
    "\n",
    "    # Determine the row at which the filter we want to match (filtermatch), peaks\n",
    "    if plot_cow or plot_gfo:\n",
    "        peak_row = lc.iloc[lc[lc['mag'] < 50.][lc['filter'] == filtermatch]['mag'].idxmin()]\n",
    "    \n",
    "    # Overplot 2018cow in the filters for which there is candidate photometry (limits)\n",
    "    if plot_cow:\n",
    "        AT2018cow = pd.read_fwf('../comparison_photometry/AT2018cow_photometry_table.dat',sep = ' ', comment = '#', header = None,\\\n",
    "            names = ['mjd', 'telescope', 'filter', 'mag', 'ABmag', 'magerr', 'source'])\n",
    "        peak_row_cow = AT2018cow.iloc[AT2018cow[AT2018cow[\"filter\"] == peak_row['filter']]['mag'].idxmin()]\n",
    "        peak_offset = peak_row['mag'] - peak_row_cow['mag']\n",
    "        mjd_offset = t0 - peak_row['jd'] + peak_row_cow['mjd']\n",
    "        for f in set(lc['filter']):\n",
    "            cow_filt = AT2018cow[AT2018cow['filter'] == f]\n",
    "            ax1.plot(cow_filt['mjd'] - mjd_offset, cow_filt['ABmag'] + peak_offset,\n",
    "                         color=color_dict[f], linestyle = ':')\n",
    "            plt.plot([],[], color = 'black', linestyle = ':', label='AT 2018cow')\n",
    "\n",
    "    # Overplot 2017gfo in the filters for which there is candidate photometry (limits)\n",
    "    if plot_gfo:\n",
    "        AT2017gfo = pd.read_csv('../comparison_photometry/AT2017gfo_optical_photometry_smartt+17.txt',sep = ' ', comment = '#',na_values='-')\n",
    "        peak_row_gfo = AT2017gfo.iloc[[AT2017gfo[peak_row['filter']].idxmin()]]\n",
    "        peak_offset = peak_row['mag'] - peak_row_gfo[peak_row['filter']]\n",
    "        mjd_offset = t0 - peak_row['jd'] + peak_row_gfo['Phase']\n",
    "        for f in set(lc['filter']):\n",
    "            nanmask = np.isfinite(AT2017gfo[f])\n",
    "            ax1.plot(AT2017gfo['Phase'][nanmask] - mjd_offset.values, AT2017gfo[f][nanmask] + peak_offset.values,\\\n",
    "                     color=color_dict[f], linestyle = '-')\n",
    "            plt.plot([],[], color = 'black', linestyle = '-', label='AT 2017gfo')\n",
    "    \n",
    "    # Overplot decline rates in g and r from Bulla models    \n",
    "    if plot_bulla:\n",
    "        peak_r = lc.iloc[lc.query(\"mag < 50. & filter == 'r'\")['mag'].idxmin()]\n",
    "        peak_g = lc.iloc[lc.query(\"mag < 50. & filter == 'g'\")['mag'].idxmin()]\n",
    "        x = np.arange(0,7,0.01)\n",
    "        ax1.plot(x + peak_r['jd'] - t0, x * 0.3 + peak_r['mag'], linestyle = '--', color = 'red', alpha=0.5)\n",
    "        ax1.plot(x + peak_g['jd'] - t0, x * 0.5 + peak_g['mag'], linestyle = '--', color = 'green', alpha=0.5)\n",
    "        plt.plot([],[], color = 'black', linestyle = '--', label='Bulla upper limits')\n",
    "\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "    #ax1.set_title(name)\n",
    "    ax1.set_xlabel(f\"Days from {xlabel}\", fontsize=18)\n",
    "    ax1.set_ylabel(\"Magnitude (AB)\", fontsize=18)\n",
    "    \n",
    "    # Legend\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = OrderedDict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys(), fontsize=16)\n",
    "\n",
    "\n",
    "    ax1.tick_params(axis='both',       # changes apply to the x-axis\n",
    "                    which='both',      # both major and minor ticks are affected\n",
    "                    labelsize=16)\n",
    "\n",
    "    # Add the discovery image as inset\n",
    "    if inset is True:\n",
    "        # Define the position\n",
    "        # Reference\n",
    "        cm = plt.cm.cubehelix\n",
    "        left, bottom, width, height = [0.51, 0.6, 0.23, 0.23]\n",
    "        ax2 = fig.add_axes([left, bottom, width, height])\n",
    "        ax2.axis('off')\n",
    "        #mean, median, std = sigma_clipped_stats(tr[:, :, 1])\n",
    "        #ax2.imshow(tr[:, :, 0], vmin = median - 0.*std, vmax = median + 4*std)\n",
    "\n",
    "        # From Tomas \n",
    "        s_max, s_min = [1.001,1.001], [1.001,1.001]\n",
    "        img_data = tr[:, :, 0]\n",
    "        imgd_scaled = np.log10(img_data)\n",
    "        vmax = np.sort(imgd_scaled[28:36,28:36].flatten())[-3]\n",
    "        npixel = (len(img_data)+1)**2\n",
    "        imgd_flat = img_data.flatten()\n",
    "        imgd_scaled[imgd_scaled<0]=np.nanmedian(img_data)\n",
    "        v_onesig = np.log10(np.nanmedian(img_data) - mad_std(imgd_flat[np.isnan(imgd_flat)==False])*1.4826)\n",
    "        vmin= max(v_onesig, np.nanmin(imgd_scaled))\n",
    "        ax2.imshow(imgd_scaled, cmap=cm, vmax=s_max[0]*vmax, vmin=vmin*s_min[0])\n",
    "        \n",
    "        '''\n",
    "        # Subtraction\n",
    "        left, bottom, width, height = [0.68, 0.6, 0.23, 0.23]\n",
    "        ax3 = fig.add_axes([left, bottom, width, height])\n",
    "        ax3.axis('off')\n",
    "        mean, median, std = sigma_clipped_stats(tr[:, :, 2])\n",
    "        ax3.imshow(tr[:, :, 2], vmin = median - 2*std, vmax = median + 3*std)\n",
    "        '''\n",
    "        \n",
    "        # Reference\n",
    "        left, bottom, width, height = [0.68, 0.6, 0.23, 0.23]\n",
    "        ax3 = fig.add_axes([left, bottom, width, height])\n",
    "        ax3.axis('off')\n",
    "        s_max, s_min = [1.001,1.001], [1.001,1.001]\n",
    "        img_data = tr[:, :, 1]\n",
    "        imgd_scaled = np.log10(img_data)\n",
    "        vmax = np.sort(imgd_scaled[28:36,28:36].flatten())[-3]\n",
    "        npixel = (len(img_data)+1)**2\n",
    "        imgd_flat = img_data.flatten()\n",
    "        imgd_scaled[imgd_scaled<0]=np.nanmedian(img_data)\n",
    "        v_onesig = np.log10(np.nanmedian(img_data) - mad_std(imgd_flat[np.isnan(imgd_flat)==False])*1.4826)\n",
    "        vmin= max(v_onesig, np.nanmin(imgd_scaled))\n",
    "        ax3.imshow(imgd_scaled, cmap=cm, vmax=s_max[0]*vmax, vmin=vmin*s_min[0])\n",
    "        \n",
    "        \n",
    "        # Add candidate name\n",
    "        ax2.text(0.8, 0.57, name, color='k', fontsize=16, ha='center', va='center', transform=ax1.transAxes)\n",
    "    \n",
    "\n",
    "    #ax1.set_xlim([0,14])\n",
    "    #ax1.set_ylim([22,18])\n",
    "    \n",
    "    if save is True or writecsv is True:\n",
    "        if forced is True:\n",
    "            forcedbool = 1\n",
    "        else:\n",
    "            forcedbool = 0\n",
    "        if stack is True:\n",
    "            stackbool = 1\n",
    "        else:\n",
    "            stackbool = 0\n",
    "        plt.savefig(f\"lc_{name}_forced{forcedbool}_stacked{stackbool}.png\")\n",
    "        if writecsv is True:\n",
    "            lc.to_csv(f\"lightcurves/lc_{name}_forced{forcedbool}_stacked{stackbool}.csv\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_xmatch_clu_glade(list_names):\n",
    "    # Filter for match in either CLU or GLADE \n",
    "    str_names = \"'\" + \"', '\".join(list_names) + \"'\"\n",
    "\n",
    "    galaxy_match_clu = pd.read_sql_query(f\"SELECT name, \\\n",
    "    clu_ra, clu_dec, clu_distmpc, clu_z, clu_dist_kpc, clu_sep_arcsec \\\n",
    "    FROM crossmatch \\\n",
    "    WHERE name in ({str_names}) and clu_ra is not NULL\", con)\n",
    "\n",
    "    galaxy_match_glade = pd.read_sql_query(f\"SELECT name, \\\n",
    "    glade_ra, glade_dec, glade_dist_mpc, glade_z, glade_dist_kpc, glade_sep_arcsec \\\n",
    "    FROM crossmatch \\\n",
    "    WHERE name in ({str_names}) and glade_ra is not NULL\", con)\n",
    "\n",
    "    return galaxy_match_clu, galaxy_match_glade\n",
    "\n",
    "def get_bgal_ebv(list_names):\n",
    "    \"\"\"\n",
    "    Get information from the db about the Galactic latitude and extinction\n",
    "    \"\"\"\n",
    "\n",
    "    str_names = \"'\" + \"', '\".join(list_names) + \"'\"\n",
    "    bgal_ebv = pd.read_sql_query(f\"SELECT name, \\\n",
    "    ra, dec, b_gal, ebv \\\n",
    "    FROM candidate \\\n",
    "    WHERE name in ({str_names})\", con)\n",
    "    \n",
    "    return bgal_ebv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "Assign or remove 'points' based on soft constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with full list, hard rejects will be removed HERE\n",
    "scoring_df = pd.read_sql_query(\"SELECT name FROM candidate where hard_reject is NULL\", con)\n",
    "\n",
    "# Define the thresholds\n",
    "thresh = {'rise': {'g': -1.0, 'r': -1., 'i': -0.5, 'sign_select': '<', 'sign_reject': '>'},\n",
    "          'fade': {'g': 0.58, 'r': 0.43, 'i': 0.30, 'sign_select': '>', 'sign_reject': '<'}\n",
    "          }\n",
    "\n",
    "# Define the filters (list)\n",
    "list_filters = ['g', 'r', 'i']\n",
    "\n",
    "# Rise, fade, or both? (list, e.g. ['rise', 'fade'])\n",
    "list_rise_fade = ['fade']\n",
    "\n",
    "scores = {'rise_select': 5,\n",
    "          'rise_pen': 0,\n",
    "          'fade_select': 10,\n",
    "          'fade_pen': -100,\n",
    "         }\n",
    "\n",
    "print(f\"Working with {len(scoring_df)} candidates\")\n",
    "print(f\"Considering the following filter(s): {list_filters}\")\n",
    "print(\"---\")\n",
    "print(\"Selected thresholds:\")\n",
    "for k1 in thresh.keys():\n",
    "    for k2 in thresh[k1].keys():\n",
    "        print(f\"{k1} {k2}: {thresh[k1][k2]}\")\n",
    "print(\"---\")\n",
    "print(\"Scoring points:\")\n",
    "for k in scores.keys():\n",
    "    print(f\"{k}: {scores[k]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter rise and fade (alerts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rise and fade for ALERTS\n",
    "for rf in list_rise_fade:\n",
    "    for f in list_filters:\n",
    "        rf_filt = pd.read_sql_query(f\"SELECT name FROM candidate WHERE index_{rf}_{f} {thresh[rf]['sign_select']} {thresh[rf][f]}\",con).values\n",
    "        print(f\"{rf}_{f}_filt_alerts: {len(rf_filt)}\" )\n",
    "\n",
    "        # Assign points if condition is met, otherwise 0\n",
    "        scoring_df[f'{rf}_{f}_filt_alerts'] = [scores[f\"{rf}_select\"] if name in rf_filt else 0 for name in scoring_df['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter rise and fade (forced phot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rise and fade for FORCED PHOTOMETRY\n",
    "for rf in list_rise_fade:\n",
    "    for f in list_filters:\n",
    "        rf_filt = pd.read_sql_query(f\"SELECT name FROM candidate WHERE index_{rf}_forced_{f} {thresh[rf]['sign_select']} {thresh[rf][f]}\",con).values\n",
    "        print(f\"{rf}_{f}_filt_forced: {len(rf_filt)}\" )\n",
    "\n",
    "        # Assign points if condition is met, otherwise 0\n",
    "        scoring_df[f'{rf}_{f}_filt_forced'] = [scores[f\"{rf}_select\"] if name in rf_filt else 0 for name in scoring_df['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter rise and fade (forced phot, STACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for rise and fade for STACKED FORCED PHOTOMETRY\n",
    "for rf in list_rise_fade:\n",
    "    for f in list_filters:\n",
    "        rf_filt = pd.read_sql_query(f\"SELECT name FROM candidate WHERE index_{rf}_stack_{f} {thresh[rf]['sign_select']} {thresh[rf][f]}\",con).values\n",
    "        print(f\"{rf}_{f}_filt_stack: {len(rf_filt)}\" )\n",
    "\n",
    "        # Assign points if condition is met, otherwise 0\n",
    "        scoring_df[f'{rf}_{f}_filt_stack'] = [scores[f\"{rf}_select\"] if name in rf_filt else 0 for name in scoring_df['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalize slow rise or fade (alerts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penalize slow rise and fade for ALERTS\n",
    "for rf in list_rise_fade:\n",
    "    for f in list_filters:\n",
    "        rf_filt = pd.read_sql_query(f\"SELECT name FROM candidate WHERE index_{rf}_{f} {thresh[rf]['sign_reject']} {thresh[rf][f]}\",con).values\n",
    "        print(f\"{rf}_{f}_pen_alerts: {len(rf_filt)}\" )\n",
    "\n",
    "        # Assign points if condition is met, otherwise 0\n",
    "        scoring_df[f'{rf}_{f}_pen_alerts'] = [scores[f\"{rf}_pen\"] if name in rf_filt else 0 for name in scoring_df['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalize slow fade (forced phot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penalize slow rise and fade for FORCED PHOTOMETRY\n",
    "for rf in list_rise_fade:\n",
    "    for f in list_filters:\n",
    "        rf_filt = pd.read_sql_query(f\"SELECT name FROM candidate WHERE index_{rf}_forced_{f} {thresh[rf]['sign_reject']} {thresh[rf][f]}\",con).values\n",
    "        print(f\"{rf}_{f}_pen_forced: {len(rf_filt)}\" )\n",
    "\n",
    "        # Assign points if condition is met, otherwise 0\n",
    "        scoring_df[f'{rf}_{f}_pen_forced'] = [scores[f\"{rf}_pen\"] if name in rf_filt else 0 for name in scoring_df['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Penalize slow fade (forced phot, STACK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penalize slow rise and fade for FORCED PHOTOMETRY\n",
    "for rf in list_rise_fade:\n",
    "    for f in list_filters:\n",
    "        rf_filt = pd.read_sql_query(f\"SELECT name FROM candidate WHERE index_{rf}_stack_{f} {thresh[rf]['sign_reject']} {thresh[rf][f]}\",con).values\n",
    "        print(f\"{rf}_{f}_pen_stack: {len(rf_filt)}\" )\n",
    "\n",
    "        # Assign points if condition is met, otherwise 0\n",
    "        scoring_df[f'{rf}_{f}_pen_stack'] = [scores[f\"{rf}_pen\"] if name in rf_filt else 0 for name in scoring_df['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Penalize candidates with long duration in forced photometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Penalize long duration transients - TOTAL\n",
    "duration_pen = pd.read_sql_query(\"SELECT name FROM candidate \\\n",
    "WHERE duration_tot > 14\", con).drop_duplicates('name').values\n",
    "print('duration_pen: ' + str(len(duration_pen)))\n",
    "\n",
    "scoring_df['duration_pen'] = [-100 if name in duration_pen else 0 for name in scoring_df['name']]\n",
    "\n",
    "# Penalize long duration transients - INDIVIDUAL FILTERS\n",
    "duration_dict = {\"g\": 10, \"r\": 12, \"i\": 14}\n",
    "for f in [\"g\", \"r\", \"i\"]:\n",
    "    duration_pen = pd.read_sql_query(f\"SELECT name FROM candidate \\\n",
    "    WHERE duration_{f} > {duration_dict[f]}\", con).drop_duplicates('name').values\n",
    "    print(f'duration_pen_{f}: ' + str(len(duration_pen)))\n",
    "    scoring_df[f'duration_pen_{f}'] = [-100 if name in duration_pen else 0 for name in scoring_df['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Crossmatch scoring\n",
    "Have ranges in assigning/penalizing points based on rising rate -> fast +1, slow-1, between 0. Ranges defined by RCF SN results, and/or 2018cow\n",
    "\n",
    "Points for candidates with more than 6 detections\n",
    "\n",
    "Point there is a non-PSF LS source with phot_z less than x\n",
    "\n",
    "Penalize if PS object within 1.5 arcsec\n",
    "\n",
    "Point is PS source with sgscore less than x and mag brighter than y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Present in either CLU or GLADE \n",
    "NOTE: GLADE crossmatch is not yet implemented, WIP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter for match in either CLU or GLADE \n",
    "galaxy_match_filt = pd.read_sql_query(\"SELECT name FROM crossmatch \\\n",
    "WHERE (clu_dist_kpc < 100) and (clu_distmpc > 10)\", con).drop_duplicates('name').values\n",
    "print('galaxy_match_filt: ' + str(len(galaxy_match_filt)))\n",
    "\n",
    "\n",
    "# Now the CLU crossmatching is giving zero points, change as desired. \n",
    "scoring_df['galaxy_match_filt'] = [1 if name in galaxy_match_filt else 0 for name in scoring_df['name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scoring_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with the results\n",
    "result_df = pd.DataFrame([])\n",
    "result_df['name'] = scoring_df['name']\n",
    "result_df['sum'] = scoring_df.sum(axis=1)\n",
    "result_df.sort_values(by='sum', ascending=False)[:12]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of the resultng scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a scoring threshold\n",
    "score_thresh = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "print(f\"There are {len(result_df[result_df['sum'] > score_thresh])} candidates above the scoring threshold\")\n",
    "\n",
    "bins = 'auto'\n",
    "#bins = np.arange(np.min(result_df['sum']), np.max(result_df['sum']), 0.5)\n",
    "bins = np.arange(-10, np.max(result_df['sum'])+1, 0.5)\n",
    "\n",
    "ax.hist(result_df['sum'], bins=bins)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel(\"Score\")\n",
    "#plt.savefig(\"score_distribution.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot up the results\n",
    "Define the list of candidates to plot. By default, it plots all those with score larger than the `score_thresh` previously defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "list_names = result_df.sort_values(by='sum', ascending=False)[result_df['sum'] > score_thresh]['name']\n",
    "\n",
    "# Get CLU and GLADE crossmatch information\n",
    "clu, glade = get_xmatch_clu_glade(list_names)\n",
    "\n",
    "# Get coords, Galactic latitude and E(B-V)\n",
    "bgal_ebv = get_bgal_ebv(list_names)\n",
    "\n",
    "list_out = []\n",
    "\n",
    "for name in list_names:\n",
    "    print(name)\n",
    "    clu_crossmatch = clu[clu['name'] == name]\n",
    "    if clu_crossmatch.empty:\n",
    "        print(\"No CLU crossmatch\")\n",
    "    else:\n",
    "        print(\"CLU crossmatch:\")\n",
    "        print(clu[clu['name'] == name])\n",
    "    print(f\"Coordinates: RA, Dec = {'{:.6f}'.format(float(bgal_ebv[bgal_ebv['name'] == name]['ra']))}, {'{:.5f}'.format(float(bgal_ebv[bgal_ebv['name'] == name]['dec']))}\")\n",
    "    print(f\"Extinction: E(B-V) = {'{:.2f}'.format(float(bgal_ebv[bgal_ebv['name'] == name]['ebv']))}\")\n",
    "    print(f\"Galactic latitude: b_Gal = {'{:.2f}'.format(float(bgal_ebv[bgal_ebv['name'] == name]['b_gal']))}\")\n",
    "        \n",
    "    ###print(glade[glade['name'] == name])\n",
    "    alerts = get_cutouts(name, username_kowalski, password_kowalski)\n",
    "    triplet = make_triplet(alerts[0])\n",
    "    plot_triplet(triplet)\n",
    "    print(f\"Alerts light curve for {name}\")\n",
    "    plot_lc(name, forced=False, stack=False, plot_alerts=True, save=False, inset=False, tr=triplet, plot_cow=False, plot_gfo=False, plot_bulla=False, filtermatch='g')    \n",
    "    print(f\"Forced photometry light curve for {name}\")\n",
    "    plot_lc(name, forced=True, stack=False, plot_alerts=True, save=False, inset=False, tr=triplet, plot_cow=False, plot_gfo=False, plot_bulla=False, filtermatch='g')\n",
    "    print(f\"Stacked forced photometry light curve for {name}\")\n",
    "    plot_lc(name, forced=True, stack=True, plot_alerts=True, save=False, inset=False, tr=triplet, plot_cow=False, plot_gfo=False, plot_bulla=False, filtermatch='g')\n",
    "    print(\"------\")\n",
    "    list_out.append(name)\n",
    "print(f\"Found {len(list_out)} candidates\")\n",
    "print(list_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****\n",
    "****\n",
    "# The End\n",
    "What you actually need for the scanning finishes here.\n",
    "****\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bonus commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate a list for kowalski query\n",
    "This are simply helpful commands that output a list of candidates already in the right format (mongodb) for checking them out on [Kowalski Lab](https://kowalski.caltech.edu/lab/ztf-alerts) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate list for kowalski\n",
    "list_names = result_df['name'][result_df['sum']> score_thresh].values\n",
    "# list_names = [name for name in list_names]\n",
    "list_names = str(list_names)\n",
    "kow = \"{'objectId': {'$in': \" + list_names.replace(' ', ', ') + \"}}\"\n",
    "print(kow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot stacking photometry for paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def get_lc_example(name, forced=True, stack=False, plot_alerts=True, save=False,\n",
    "                   tr=None, writecsv=False):\n",
    "    '''Get the light curve of a candidate'''\n",
    "\n",
    "    if forced is False:\n",
    "        plot_alerts = True\n",
    "        lc = pd.DataFrame(columns=['jd', 'mag', 'mag_unc', 'filter', 'limmag', 'forced'])\n",
    "    else:\n",
    "        if stack is True:\n",
    "            table = 'lightcurve_stacked'        \n",
    "        elif forced is True:\n",
    "            table = 'lightcurve_forced'\n",
    "        lc = pd.read_sql_query(f\"SELECT jd, mag, mag_unc, filter, limmag FROM {table} \\\n",
    "WHERE name = '{name}'\", con)\n",
    "        lc[\"forced\"] = np.ones(len(lc))\n",
    "\n",
    "    if plot_alerts is True:\n",
    "        alerts = pd.read_sql_query(f\"SELECT jd, magpsf, sigmapsf, filter FROM lightcurve \\\n",
    "WHERE name = '{name}'\", con)\n",
    "        # FIXME REMOVE!!!!!!!!\n",
    "        if len(alerts) > 20:\n",
    "            print(\"TOO MANY ALERTS!!!! Not plotting them\")\n",
    "        for i, a in alerts.iterrows():\n",
    "            # FIXME REMOVE!!!!!!!!!!!\n",
    "            if len(alerts) > 20:\n",
    "                continue\n",
    "            # If the time difference between the alert and any forced phot is >5min, consider the alert\n",
    "            if lc.empty or np.min(np.abs(np.array(lc['jd']) - np.array(a['jd']))) > 5./60/60/24.:\n",
    "                #print(f\"Adding an alert for {name}\")\n",
    "                new_row = [a['jd'], a['magpsf'], a['sigmapsf'], a[\"filter\"], 99.0, 0]\n",
    "                new_row = pd.DataFrame([new_row], columns=['jd', 'mag', 'mag_unc', 'filter', 'limmag', 'forced'])\n",
    "                lc = lc.append([lc, new_row], ignore_index=True)\n",
    "\n",
    "    return lc\n",
    "\n",
    "###############\n",
    "\n",
    "name = \"ZTF19abcpiag\"\n",
    "\n",
    "# Initialize the figure\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(9,12), sharex=True)\n",
    "\n",
    "for ax, doforced, stack, ylim, phot, pos in zip([ax1, ax2, ax3], [False, True, True],\n",
    "                                     [False, False, True],\n",
    "                                    [[21,19], [22,19], [23,19]],\n",
    "                                    [\"Alerts\", \"Forced photometry\", \"Stacked forced photometry\"],\n",
    "                                    [0.9, 0.8, 0.73]):\n",
    "\n",
    "    save = False\n",
    "    writecsv = False\n",
    "    \n",
    "    color_dict = {'g': 'green', 'r': 'red', 'i': 'y'}\n",
    "    forced_dict = {'1': 's', '0': 'o'}\n",
    "    \n",
    "    # Get the light curve\n",
    "    if phot == \"Alerts\":\n",
    "        lc = pd.read_csv(\"lc_ZTF19abcpiag_alerts.csv\")\n",
    "        lc = lc.rename(columns={\"magpsf\": \"mag\", \"sigmamagpsf\": \"mag_unc\", \"jdobs\": \"jd\"})\n",
    "        lc[\"forced\"] = np.zeros(len(lc))\n",
    "\n",
    "    else:\n",
    "        lc = get_lc_example(name, forced=doforced, stack=stack, plot_alerts=False, save=False, tr=None, writecsv=False )\n",
    "\n",
    "    # Starting time\n",
    "    t0 = np.min(lc[lc['mag'] < 50]['jd'].values)\n",
    "    xlabel = Time(t0, format='jd').iso[0:19]\n",
    "    \n",
    "    for f in set(lc['filter']):\n",
    "        tf = lc[lc['filter'] == f]\n",
    "        tf_det = tf[tf['mag'] < 50.]\n",
    "        tf_ul = tf[tf['mag'] > 50.]\n",
    "        for isforced in [0,1]:\n",
    "            if isforced == 0:\n",
    "                label = f\"{f} alerts\"\n",
    "            else:\n",
    "                if stack is True:\n",
    "                    label = f\"{f} forced phot stacked\"\n",
    "                else:\n",
    "                    label = f\"{f} forced phot\"\n",
    "            tf_det2 = tf_det[tf_det['forced'] == isforced]\n",
    "            if len(tf_det2) == 0:\n",
    "                continue\n",
    "            ax.errorbar(tf_det2['jd'].values - t0, tf_det2['mag'], yerr=tf_det2['mag_unc'],\n",
    "                         color=color_dict[f], markeredgecolor='k',\n",
    "                         fmt=forced_dict[str(int(isforced))], label=label)\n",
    "        if len(tf_ul) != 0:\n",
    "            ax.errorbar(tf_ul['jd'].values - t0, tf_ul['limmag'], markeredgecolor=color_dict[f],\n",
    "                         markerfacecolor='w', fmt='v')\n",
    "            plt.plot([],[], 'kv', markeredgecolor='k', markerfacecolor='w', label='upper limits')\n",
    "        ax.tick_params(axis='both',       # changes apply to the x-axis\n",
    "                        which='both',      # both major and minor ticks are affected\n",
    "                        labelsize=16)\n",
    "        ax.set_ylabel(\"Magnitude (AB)\", fontsize=18)\n",
    "        ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "        #ax.legend(by_label.values(), by_label.keys(), fontsize=16, loc='lower right')\n",
    "        \n",
    "        # Add name\n",
    "        ax.text(pos, 0.85, phot, color='k', fontsize=18, ha='center', va='center', transform=ax.transAxes)\n",
    "\n",
    "        ax.set_ylim(ylim)\n",
    "        ax.grid(linestyle='dotted')\n",
    "    \n",
    "    \n",
    "    if save is True or writecsv is True:\n",
    "        if forced is True:\n",
    "            forcedbool = 1\n",
    "        else:\n",
    "            forcedbool = 0\n",
    "        if stack is True:\n",
    "            stackbool = 1\n",
    "        else:\n",
    "            stackbool = 0\n",
    "        plt.savefig(f\"lc_{name}_forced{forcedbool}_stacked{stackbool}.png\")\n",
    "        if writecsv is True:\n",
    "            lc.to_csv(f\"../lc_paper/lc_{name}_forced{forcedbool}_stacked{stackbool}.csv\")\n",
    "\n",
    "#ax1.set_title(name)\n",
    "#ax.set_xlabel(f\"Days from {xlabel}\", fontsize=18)\n",
    "ax.set_xlabel(f\"Time from first detection (days)\", fontsize=18)\n",
    "ax.set_xlim([-3.5, 6.2])\n",
    "\n",
    "#plt.gca().invert_yaxis()\n",
    "    \n",
    "# Legend\n",
    "#handles, labels = plt.gca().get_legend_handles_labels()\n",
    "#by_label = OrderedDict(zip(labels, handles))\n",
    "\n",
    "plt.savefig(f\"lc_example_stacked_{name}.pdf\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Close the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
